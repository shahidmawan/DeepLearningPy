{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e69aa19d-661d-4d1e-a00f-dc37834c76e2",
    "_uuid": "b8f1c58eac75b7dfb070b4f1ea9c4fb602cf8383",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import cv2\n",
    "import scipy\n",
    "from skimage import io\n",
    "\n",
    "from PIL import ImageFile\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "112a0b99-6c71-42c6-9e18-40ea35e47cba",
    "_uuid": "a32fe39e4e445e4aba5bba40f9a9eaff4a6ccbae",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "322a5a33-ac1c-4d0e-9985-bd68b6181dae",
    "_uuid": "c7b648939c7efb2885c29b1c8f754e67a6afb1dd"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image as keras_image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input as inception_v3_preprocess_input\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8b136c5d-5bc0-4b0e-97fc-8420d254cc68",
    "_uuid": "290901b177483899630d69baf2ed9dca2b1ef530",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, BatchNormalization\n",
    "from keras.layers import Activation, Flatten, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d02dd8bd-fee2-4ff2-8d59-ef1baeb6a5e1",
    "_uuid": "b406e62e3a5b857675663e68b6d5d2c6d3d85551",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_plot(fit_history):\n",
    "    plt.figure(figsize=(18, 4))\n",
    "\n",
    "    plt.plot(fit_history.history['loss'], label = 'train')\n",
    "    plt.plot(fit_history.history['val_loss'], label = 'test')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title('Loss Function');  \n",
    "\n",
    "def acc_plot(fit_history):\n",
    "    plt.figure(figsize=(18, 4))\n",
    "\n",
    "    plt.plot(fit_history.history['acc'], label = 'train')\n",
    "    plt.plot(fit_history.history['val_acc'], label = 'test')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy');\n",
    "    \n",
    "def path_to_tensor(img_path):\n",
    "    img = keras_image.load_img(\"../input/flower-color-images/flower_images/flower_images/\"+img_path, \n",
    "                               target_size=(128, 128))\n",
    "    x = keras_image.img_to_array(img)\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3d34621c-b693-41c1-aded-f17d311ee995",
    "_uuid": "0ddeae02bbe9948380a58f2ca248e91030b9bb2c"
   },
   "source": [
    "# &#x1F310; &nbsp; 1. Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "11dc0093-9e0d-445a-94b8-9d775a1c7b9a",
    "_uuid": "59536cf328dcdace6ed6225a270add5ae3700269"
   },
   "outputs": [],
   "source": [
    "flowers = pd.read_csv(\"../input/flower-color-images/flower_images/flower_images/flower_labels.csv\")\n",
    "flower_files = flowers['file']\n",
    "flower_targets = flowers['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "17b4a6db4652dc403473480725a75d8b75048da3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with h5py.File('../input/FlowerColorImages.h5', 'r') as f:\n",
    "#     flower_tensors = f['images'].value\n",
    "#     flower_targets = f['labels'].value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eadd3ca9-04ae-435b-9b89-2c0fd493a470",
    "_uuid": "1e5f44cb08f50540d7b3812df6267e70259d6345"
   },
   "outputs": [],
   "source": [
    "print('Label: ', flower_targets[168])\n",
    "flower_image = cv2.imread(\"../input/flower-color-images/flower_images/flower_images/\"+flower_files[168])\n",
    "rgb_flower_image = cv2.cvtColor(flower_image, cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(rgb_flower_image);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a61a8e68-b073-4488-b34b-b1d313c61774",
    "_uuid": "042af292fb8ed20d7011cd748eaacfae1ea7a0fe"
   },
   "outputs": [],
   "source": [
    "flower_tensors = paths_to_tensor(flower_files);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ecaf5e5b-d83b-4c80-a2e2-692ff86f1b1e",
    "_uuid": "0abca232b35f545a13252e80856f7429b28b5d0a"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(flower_tensors, flower_targets, \n",
    "                                                    test_size = 0.2, random_state = 1)\n",
    "[x_train.shape, y_train.shape, x_test.shape, y_test.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d44f455354b68703ed485fab34131af1d5b20106"
   },
   "outputs": [],
   "source": [
    "n = int(len(x_test)/2)\n",
    "x_valid, y_valid = x_test[:n], y_test[:n]\n",
    "x_test, y_test = x_test[n:], y_test[n:]\n",
    "x_train.shape, x_test.shape, x_valid.shape, y_train.shape, y_test.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "861f7c31-21bf-4a6f-a983-53b6762e8962",
    "_uuid": "73c16b55e7ceb3bf802dd173687f59c824c3f24e"
   },
   "outputs": [],
   "source": [
    "print('Label: ', y_train[1])\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow((x_train[1]/255).reshape(128,128,3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e50c4660-9075-4c1e-a184-82e60a868401",
    "_uuid": "30c216a7bc9a8fdabccd12a0c0a0a976006703c1"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')/255\n",
    "x_test = x_test.astype('float32')/255\n",
    "x_valid = x_valid.astype('float32')/255\n",
    "\n",
    "c_y_train = to_categorical(y_train, 10)\n",
    "c_y_test = to_categorical(y_test, 10)\n",
    "c_y_valid = to_categorical(y_valid, 10)\n",
    "\n",
    "[x_train.shape, c_y_train.shape, x_test.shape, c_y_test.shape, x_valid.shape, c_y_valid.shape]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d9d751a9-8b26-4c4b-916c-e515a09c5c07",
    "_uuid": "90acd171f4d7624e7c6c0d1954b15c22438df057"
   },
   "source": [
    "# &#x1F310; &nbsp; 2. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a99c4ea9-377a-4ba5-ad28-72062e056a8e",
    "_uuid": "97702030f40e1bf3487ea7b80719ff632de988b4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN\n",
    "def cnn_mc_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, (5, 5), padding='same', input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(96, (5, 5)))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Flatten())\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    \n",
    "    model.add(Dense(512, activation='tanh'))\n",
    "    model.add(Dropout(0.25)) \n",
    "    \n",
    "#    model.add(Dense(256, activation='tanh'))\n",
    "#    model.add(Dropout(0.25)) \n",
    "    \n",
    "    model.add(Dense(128, activation='tanh'))\n",
    "    model.add(Dropout(0.25)) \n",
    "\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "cnn_mc_model = cnn_mc_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cc051224-ec27-455e-b1ba-a29115538837",
    "_kg_hide-output": true,
    "_uuid": "cf6f60213a63a29e370dff2dd6a5f5ddadecb7fa"
   },
   "outputs": [],
   "source": [
    "cnn_mc_history = cnn_mc_model.fit(x_train, c_y_train, \n",
    "                                  epochs=50, batch_size=64, verbose=2,\n",
    "                                  validation_data=(x_valid, c_y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "510cbfd5-301a-4420-9cc4-0751b224e1c3",
    "_uuid": "7fe116b76d2b57ec84c13af0b6b5907736afe21c"
   },
   "outputs": [],
   "source": [
    "loss_plot(cnn_mc_history)\n",
    "acc_plot(cnn_mc_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5c019add1263d9a6fff670f4eea0add7ed3dd625"
   },
   "outputs": [],
   "source": [
    "cnn_mc_test_score = cnn_mc_model.evaluate(x_test, c_y_test)\n",
    "cnn_mc_test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "4647a8c1ea8e601890d73410712e4a5eac5ef2dc"
   },
   "outputs": [],
   "source": [
    "data_generator = keras_image.ImageDataGenerator(shear_range=0.3, \n",
    "                                                zoom_range=0.3,\n",
    "                                                rotation_range=30,\n",
    "                                                horizontal_flip=True)\n",
    "cnn_mc_dg_history = cnn_mc_model.fit_generator(data_generator.flow(x_train, c_y_train, batch_size=64),\n",
    "                                               steps_per_epoch=189, epochs=3, verbose=2, \n",
    "                                               validation_data=(x_valid, c_y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f2fc03f526f6fce83309a4a465be74289ff7bc37"
   },
   "outputs": [],
   "source": [
    "cnn_mc_test_score = cnn_mc_model.evaluate(x_test, c_y_test)\n",
    "cnn_mc_test_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
